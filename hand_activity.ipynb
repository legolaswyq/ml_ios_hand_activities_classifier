{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import glob \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_dict(label_file):\n",
    "    dict = {}\n",
    "    start_idx = 0\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            dict[line.strip()] = start_idx\n",
    "            start_idx += 1\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ds(data_dir, label_dict):\n",
    "    files = glob.glob(f\"{data_dir}/**/*.csv\")\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    for file in files:\n",
    "        parent_dir = os.path.basename(os.path.dirname(file))\n",
    "        csv_data = pd.read_csv(file)\n",
    "        csv_data = csv_data.drop(columns=[\"timestamp\"])\n",
    "        n = csv_data.shape[0]\n",
    "        indices = np.linspace(0, n-1, num=100, dtype=int)\n",
    "        new_csv_data = csv_data.iloc[indices]\n",
    "        train_data.append(new_csv_data)\n",
    "        train_labels.append(label_dict[parent_dir])\n",
    "\n",
    "    return train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "label_file = \"labels.txt\"\n",
    "label_dict = create_labels_dict(label_file)\n",
    "\n",
    "\n",
    "train_dir = \"data/train\"\n",
    "train_data, train_labels = prepare_ds(train_dir, label_dict)\n",
    "train_data = np.array(train_data)\n",
    "train_data = np.vstack(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "\n",
    "val_dir = \"data/val\"\n",
    "val_data, val_labels = prepare_ds(val_dir, label_dict)\n",
    "val_data = np.array(val_data)\n",
    "val_data = np.vstack(val_data)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "\n",
    "train_data = train_data.reshape(-1, 100, 6)\n",
    "val_data = val_data.reshape(-1, 100, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HandActivitiesDataset(Dataset):\n",
    "    def __init__(self, dataDir, labels_dict, transform=None):\n",
    "        self.dataDir = dataDir\n",
    "        self.label_dict = labels_dict\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.load_data()\n",
    "        \n",
    "\n",
    "    def load_data(self):\n",
    "        files = glob.glob(f\"{self.dataDir}/**/*.csv\")\n",
    "        for file in files:\n",
    "            parent_dir = os.path.basename(os.path.dirname(file))\n",
    "            csv_data = pd.read_csv(file)\n",
    "            csv_data = csv_data.drop(columns=[\"timestamp\"])\n",
    "            n = csv_data.shape[0]\n",
    "            indices = np.linspace(0, n-1, num=100, dtype=int)\n",
    "            new_csv_data = csv_data.iloc[indices]\n",
    "            self.data.append(new_csv_data)\n",
    "            self.labels.append(self.label_dict[parent_dir])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.tensor(train_data).float()\n",
    "train_y = torch.tensor(train_labels)\n",
    "train_y = F.one_hot(train_y, num_classes=3)\n",
    "val_X = torch.tensor(val_data).float()\n",
    "val_y = torch.tensor(val_labels)\n",
    "val_y = F.one_hot(val_y, num_classes=3)\n",
    "\n",
    "train_ds = TensorDataset(train_X, train_y)\n",
    "val_ds = TensorDataset(val_X, val_y)\n",
    "\n",
    "train_ds = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_ds = DataLoader(val_ds, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, seq_len, dropout=0.5, output_size=3 ):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # LSTM Layers\n",
    "        self.lstm_1 = nn.LSTM(input_size, hidden_sizes[0], num_layers=2,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_21 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[1], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_22 = nn.LSTM(input_size, hidden_sizes[1], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_31 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[2], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_32 = nn.LSTM(4*hidden_sizes[1], hidden_sizes[2], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_41 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[3], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_42 = nn.LSTM(4*hidden_sizes[2], hidden_sizes[3], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        hidd = 2*hidden_sizes[0] + 4*(hidden_sizes[1]+hidden_sizes[2]+hidden_sizes[3])\n",
    "        self.lstm_5 = nn.LSTM(hidd, hidden_sizes[4], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Sequential(nn.Linear(2*hidden_sizes[4]*seq_len, 4096),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(4096, 1024),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(1024, output_size),\n",
    "                                nn.Sigmoid()\n",
    "                               )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm layers:\n",
    "        x1, _ = self.lstm_1(x)\n",
    "        \n",
    "        x_x1, _ = self.lstm_21(x1)\n",
    "        x_x2, _ = self.lstm_22(x)\n",
    "        x2 = torch.cat([x_x1, x_x2], dim=2)\n",
    "        \n",
    "        x_x1, _ = self.lstm_31(x_x1)\n",
    "        x_x2, _ = self.lstm_32(x2)\n",
    "        x3 = torch.cat([x_x1, x_x2], dim=2)\n",
    "        \n",
    "        x_x1, _ = self.lstm_41(x_x1)\n",
    "        x_x2, _ = self.lstm_42(x3)\n",
    "        x4 = torch.cat([x_x1, x_x2], dim=2)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=2)\n",
    "        x, _ = self.lstm_5(x)\n",
    "        \n",
    "        # fully connected layers:\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALIDATION FUNCTION\n",
    "def validation(model, loader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    preds_all = torch.FloatTensor()\n",
    "    labels_all = torch.FloatTensor()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, labels in loader:\n",
    "            labels_all = torch.cat((labels_all, labels), dim=0)\n",
    "            batch_x, labels = batch_x.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            \n",
    "            output = model.forward(batch_x)\n",
    "            loss += criterion(output,labels).item()\n",
    "            preds_all = torch.cat((preds_all, output.to(\"cpu\")), dim=0)\n",
    "    total_loss = loss/len(loader)\n",
    "    auc_score = roc_auc_score(labels_all, preds_all)\n",
    "    return total_loss, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, validloader, criterion, optimizer, \n",
    "                scheduler, epochs=20, device=\"cpu\", print_every=1):\n",
    "    model.to(device)\n",
    "    best_auc = 0\n",
    "    best_epoch = 0\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for batch_x, labels in trainloader:\n",
    "            batch_x, labels = batch_x.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            \n",
    "            # Training \n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(batch_x)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        # at the end of each epoch calculate loss and auc score:\n",
    "        model.eval()\n",
    "        train_loss, train_auc = validation(model, trainloader, criterion, device)\n",
    "        valid_loss, valid_auc = validation(model, validloader, criterion, device)\n",
    "        if valid_auc > best_auc:\n",
    "            best_auc = valid_auc\n",
    "            best_epoch = e\n",
    "            torch.save(model.state_dict(), \"best-state.pt\")\n",
    "        if e % print_every == 0:\n",
    "            to_print = \"Epoch: \"+str(e+1)+\" of \"+str(epochs)\n",
    "            to_print += \".. Train Loss: {:.4f}\".format(train_loss)\n",
    "            to_print += \".. Valid Loss: {:.4f}\".format(valid_loss)\n",
    "            to_print += \".. Valid AUC: {:.3f}\".format(valid_auc)\n",
    "            print(to_print)\n",
    "    # After Training:\n",
    "    model.load_state_dict(torch.load(\"best-state.pt\"))\n",
    "    to_print = \"\\nTraining completed. Best state dict is loaded.\\n\"\n",
    "    to_print += \"Best Valid AUC is: {:.4f} after {} epochs\".format(best_auc,best_epoch+1)\n",
    "    print(to_print)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION FUNCTION\n",
    "def prediction(model, loader, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds_all = torch.FloatTensor()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            \n",
    "            output = model.forward(batch_x).to(\"cpu\")\n",
    "            preds_all = torch.cat((preds_all, output), dim=0)\n",
    "    return preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [288, 192, 144, 96, 32] 100\n",
      "Model: \n",
      "RNN(\n",
      "  (lstm_1): LSTM(6, 288, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_21): LSTM(576, 192, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_22): LSTM(6, 192, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_31): LSTM(384, 144, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_32): LSTM(768, 144, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_41): LSTM(288, 96, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_42): LSTM(576, 96, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (lstm_5): LSTM(2304, 32, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=1024, out_features=3, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [288, 192, 144, 96, 32]\n",
    "max_learning_rate = 0.001\n",
    "epochs = 41\n",
    "input_size = 6\n",
    "sequence_length = 100\n",
    "# Model\n",
    "print(input_size, hidden_sizes, sequence_length)\n",
    "model_lstm = RNN(input_size, hidden_sizes, sequence_length)\n",
    "print(\"Model: \")\n",
    "print(model_lstm)\n",
    "\n",
    "# criterion, optimizer, scheduler\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=max_learning_rate)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                          max_lr = max_learning_rate,\n",
    "                                          epochs = epochs,\n",
    "                                          steps_per_epoch = len(train_ds),\n",
    "                                          pct_start = 0.2,\n",
    "                                          anneal_strategy = \"cos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([6, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 54\u001b[0m\n\u001b[1;32m     49\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_ds))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(preds)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(labels_all)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds)\n\u001b[1;32m     56\u001b[0m acc \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 54\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_ds))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(preds)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(labels_all)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, labels_all) \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m==\u001b[39m b)\n\u001b[1;32m     55\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds)\n\u001b[1;32m     56\u001b[0m acc \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    my_device = \"cuda\"\n",
    "    print(\"GPU is enabled\")\n",
    "else:\n",
    "    my_device = \"cpu\"\n",
    "    print(\"No GPU :(\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "model_lstm.to(my_device)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    model_lstm.train()\n",
    "    for inputs, labels in train_ds:\n",
    "        inputs, labels = inputs.to(my_device).float(), labels.to(my_device).float()\n",
    "        # print(labels.shape)\n",
    "        # labels = labels.unsqueeze(1).float()\n",
    "        # print(labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model_lstm(inputs)\n",
    "        # output = output.unsqueeze(1)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_ds))\n",
    "    \n",
    "    model_lstm.eval()\n",
    "    preds = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_ds:\n",
    "            inputs, labels = inputs.to(my_device).float(), labels.to(my_device).float()\n",
    "            output = model_lstm(inputs)\n",
    "            pred = torch.argmax(output.to(\"cpu\"), axis=1)\n",
    "            label = torch.argmax(labels.to(\"cpu\"))\n",
    "            # print(label)\n",
    "            # print(pred)\n",
    "            preds.append(pred)\n",
    "            labels_all.append(label)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            val_loss += loss.item()\n",
    "        val_losses.append(val_loss/len(val_ds))\n",
    "    \n",
    "\n",
    "    # print(preds)\n",
    "    # print(labels_all)\n",
    "    correct = sum(1 for a, b in zip(preds, labels_all) if a == b)\n",
    "    total = len(preds)\n",
    "    acc = correct / total * 100\n",
    "        # auc_score = roc_auc_score(labels_all, preds_all)\n",
    "    print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
    "          \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "          \"Validation Loss: {:.3f}.. \".format(val_losses[-1]),\n",
    "          \"Validation AUC: {:.3f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled\n",
      "Epoch: 1/41..  Training Loss: 0.000..  Validation Loss: 0.028..  Validation Accuracy: 93.33%\n",
      "Epoch: 2/41..  Training Loss: 0.000..  Validation Loss: 0.027..  Validation Accuracy: 94.00%\n",
      "Epoch: 3/41..  Training Loss: 0.000..  Validation Loss: 0.027..  Validation Accuracy: 94.67%\n",
      "Epoch: 4/41..  Training Loss: 0.000..  Validation Loss: 0.027..  Validation Accuracy: 94.67%\n",
      "Epoch: 5/41..  Training Loss: 0.000..  Validation Loss: 0.026..  Validation Accuracy: 94.67%\n",
      "Epoch: 6/41..  Training Loss: 0.000..  Validation Loss: 0.026..  Validation Accuracy: 94.67%\n",
      "Epoch: 7/41..  Training Loss: 0.000..  Validation Loss: 0.026..  Validation Accuracy: 95.33%\n",
      "Epoch: 8/41..  Training Loss: 0.000..  Validation Loss: 0.025..  Validation Accuracy: 95.33%\n",
      "Epoch: 9/41..  Training Loss: 0.000..  Validation Loss: 0.025..  Validation Accuracy: 95.33%\n",
      "Epoch: 10/41..  Training Loss: 0.000..  Validation Loss: 0.025..  Validation Accuracy: 95.33%\n",
      "Epoch: 11/41..  Training Loss: 0.000..  Validation Loss: 0.024..  Validation Accuracy: 95.33%\n",
      "Epoch: 12/41..  Training Loss: 0.000..  Validation Loss: 0.024..  Validation Accuracy: 95.33%\n",
      "Epoch: 13/41..  Training Loss: 0.000..  Validation Loss: 0.024..  Validation Accuracy: 95.33%\n",
      "Epoch: 14/41..  Training Loss: 0.000..  Validation Loss: 0.024..  Validation Accuracy: 95.33%\n",
      "Epoch: 15/41..  Training Loss: 0.000..  Validation Loss: 0.024..  Validation Accuracy: 95.33%\n",
      "Epoch: 16/41..  Training Loss: 0.000..  Validation Loss: 0.023..  Validation Accuracy: 96.00%\n",
      "Epoch: 17/41..  Training Loss: 0.000..  Validation Loss: 0.023..  Validation Accuracy: 96.00%\n",
      "Epoch: 18/41..  Training Loss: 0.000..  Validation Loss: 0.023..  Validation Accuracy: 96.00%\n",
      "Epoch: 19/41..  Training Loss: 0.000..  Validation Loss: 0.023..  Validation Accuracy: 96.00%\n",
      "Epoch: 20/41..  Training Loss: 0.000..  Validation Loss: 0.023..  Validation Accuracy: 96.00%\n",
      "Epoch: 21/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 22/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 23/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 24/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 25/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 26/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 27/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 28/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 29/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 30/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 31/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.00%\n",
      "Epoch: 32/41..  Training Loss: 0.000..  Validation Loss: 0.022..  Validation Accuracy: 96.67%\n",
      "Epoch: 33/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 34/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 35/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 36/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 37/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 38/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 39/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 40/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n",
      "Epoch: 41/41..  Training Loss: 0.000..  Validation Loss: 0.021..  Validation Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    my_device = \"cuda\"\n",
    "    print(\"GPU is enabled\")\n",
    "else:\n",
    "    my_device = \"cpu\"\n",
    "    print(\"No GPU :(\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model_lstm.to(my_device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    model_lstm.train()\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in train_ds:\n",
    "        inputs, labels = inputs.to(my_device).float(), labels.to(my_device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model_lstm(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_ds))\n",
    "\n",
    "    # Validation loop\n",
    "    model_lstm.eval()\n",
    "    preds = []\n",
    "    labels_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_ds:\n",
    "            inputs, labels = inputs.to(my_device).float(), labels.to(my_device).float()\n",
    "            output = model_lstm(inputs)\n",
    "\n",
    "            # Get predicted and true labels\n",
    "            pred = torch.argmax(output, dim=1).cpu()  # Convert to CPU\n",
    "            label = torch.argmax(labels, dim=1).cpu()  # Convert to CPU\n",
    "\n",
    "            preds.append(pred)\n",
    "            labels_all.append(label)\n",
    "\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_ds))\n",
    "\n",
    "    # Convert lists to tensors for accuracy calculation\n",
    "    preds = torch.cat(preds)\n",
    "    labels_all = torch.cat(labels_all)\n",
    "\n",
    "    correct = torch.eq(preds, labels_all).sum().item()\n",
    "    total = labels_all.size(0)\n",
    "    acc = (correct / total) * 100  # Convert to percentage\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch: {}/{}.. \".format(epoch + 1, epochs),\n",
    "          \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "          \"Validation Loss: {:.3f}.. \".format(val_losses[-1]),\n",
    "          \"Validation Accuracy: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
